oss.getCCC(KeeneSOC)
#' @return data.frame
#' @export
#'
#' @examples
#' #Calculate fit statistics between 2 numeric vectors
#' observed<- c(3,7,4,9,6,2)
#' predicted<-c(2,5,4,7,3,5)
#' oss.goof(observed,predicted)
#'
#'
oss.goof <- function(predicted,observed){
#pull in data and create a dataframe, remove NAs, assign data to objects
data<- stats::na.omit(cbind(as.data.frame(observed),as.data.frame(predicted)))
observed <-data$observed
predicted<-data$predicted
#calculate r square and adjusted r square
rLM<- stats::lm(predicted~observed)
adj.r2 <- as.matrix(summary(rLM)$adj.r.squared)
r2 <- as.matrix(summary(rLM)$r.squared)
#calculate the root mean square error
SEP<- mean((observed-predicted)^2)
RMSE<- sqrt(SEP)
#calculate the mean absolute error
MAE<- sum(abs(observed-predicted))/nrow(data)
#calculate the bias of the estimate
bias <- (mean(mean(predicted)-observed))
#calculate Lin's Concordance correlation coefficient
ux<-mean(observed)
uy<-mean(predicted)
varxy<-mean((observed-ux) * (predicted-uy))
varx<-sum((predicted-mean(predicted))^2)/(length(predicted)-1)
vary<-sum((observed-mean(observed))^2)/(length(observed)-1)
concordance <- (2*varxy)/(varx+vary+(ux-uy)^2)
#assign the outputs to a dataframe and print it in the console for viewing
goof <-data.frame(r2=c(r2),adj.r2=c(adj.r2),concordance=c(concordance),RMSE=c(RMSE),bias=c(bias),MAE=c(MAE))
}
oss.getCCC(KeeneSOC)
library(devtools)
devtools::check()
devtools::check()
?capture.output
devtools::check()
?summarize
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()
data(KeeneSOC)
x<- KeeneSOC$pred
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
View(df)
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
View(df)
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
View(df)
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(df)
View(final_pars)
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
names(Model$bestTune)
names(x$bestTune)
Model<- KeeneSOC
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(final_pars)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
#.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
Model<- Model
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
devtools::check()
devtools::check()
library(devtools)
devtools::check()
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
sum(prob2)
barplot(prob1)
barplot(prob2)
kl_divergence(p=prob1, q=prob1, type='prob', unit='log2')
kl_divergence(p=prob1, q=prob2, type='prob', unit='log2')
kl_divergence(p=prob2, q=prob1, type='prob', unit='log2')
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
barplot(prob1)
barplot(prob2)
oss.jsd(p=prob1, q=prob1, type='prob', unit='log2')
oss.jsd(p=prob1, q=prob2, type='prob', unit='log2')
oss.jsd(p=prob2, q=prob1, type='prob', unit='log2')
## KL Divergence for Continuous data - Normal Distribution
kl_cont<- function(p,q){
d1<- as.vector(p)
d2<- as.vector(q)
sigma.1<- var(d1, na.rm=TRUE)
sigma.2<- var(d2, na.rm=TRUE)
mu.1<- mean(d1, na.rm=TRUE)
mu.2<- mean(d2, na.rm=TRUE)
k<- log(sigma.2/sigma.1) + (sigma.1^2 + (mu.1 - mu.2)^2)/(2* sigma.2^2) - 0.5
return(k)
}
# we can try with theoretical data
ref<- rnorm(2000, mean=100, sd=1)
dat1<- ref + 1
dat2<- ref + 2
dat3<- ref + 4
dat4<- ref + 8
dat5<- ref + 16
d<- density(ref)
e<- density(dat1)
f<- density(dat2)
g<- density(dat3)
h<- density(dat4)
i<- density(dat5)
plot(d, xlim=c(90,125))
lines(e, col='red')
lines(f, col='blue')
lines(g, col='blue')
lines(h, col='blue')
lines(i, col='blue')
t<- rbind(kl_cont(ref, dat1),
kl_cont(ref, dat2),
kl_cont(ref, dat3),
kl_cont(ref, dat4),
kl_cont(ref, dat5))
plot(c(1,2,4,8,16),t)
devtools::check()
#' barplot(prob2)
#' # we now run the JS Divergence
#' # it is a symmetrical test, which means Q|P == P|Q
#' # if we enter the same distribution as both P and Q, we confirm a score of 0 or no divergence
#' oss.jsdist(p=prob1, q=prob1, type='prob', unit='log2')
#' # P|Q
#' oss.jsdist(p=prob1, q=prob2, type='prob', unit='log2')
#' # Q|P
#' oss.jsdist(p=prob2, q=prob1, type='prob', unit='log2')
#'
oss.jsdist<- function(p, q, type=NULL, unit='log2'){
# check to ensure user has specified data type as 'prob' or 'count' and stop function if not set
if(is.null(type)) {stop('The function argument type must be either prob or count')
}
else if (type=='prob'){
p<- p
q<- q
}
else {
p<- p/sum(p)
q<- q/sum(q)
}
m <- 0.5 * (p + q)
js<- 0.5 * oss.kld(p, m, type, unit) + 0.5 * oss.kld(q, m, type, unit)
js<- sqrt(js)
return(js)
}
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
sum(prob2)
barplot(prob1)
barplot(prob2)
oss.jsdist(p=prob1, q=prob1, type='prob', unit='log2')
# P|Q
oss.jsdist(p=prob1, q=prob2, type='prob', unit='log2')
# Q|P
oss.jsdist(p=prob2, q=prob1, type='prob', unit='log2')
devtools::check()
devtools::check()
load("~/R/dev/onsoilsurvey/data/keene.rda")
plot(keene)
keene<- keene
keene<- keene+1
keene
usethis::use_data(keene)
devtools::check()
devtools::document(keene)
devtools::document()
devtools::document()
devtools::check()
tools::checkRdaFiles()
tools::checkRdaFiles('data')
profiles<- read.csv('C:/Users/atiko/Desktop/OMAFRA_Code/01_InProgress/profiles.csv')
save(profiles, file = "/data/profiles.rda")
save(profiles, file = paste0(getwd(),"/data/profiles.rda"))
rm(profiles)
devtools::check()
data(profiles)
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::document(profiles)
devtools::document('profiles')
devtools::document('data/profiles')
devtools::document('data/profiles.rda')
load("~/R/dev/onsoilsurvey/data/profiles.rda")
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
library(onsoilsurvey)
data(profiles)
oss.optspline(obj=profiles, var.name="pH_CaCl2",
lam= c(1, 0.1, 0.01), d=c(0,5,15,30,60,100),
vlow= 0, vhigh= 9, save.plots=FALSE)
oss.optspline(obj=profiles, var.name="pH_CaCl2",
lam= c(1, 0.1, 0.01), d=t(c(0,5,15,30,60,100)),
vlow= 0, vhigh= 9, save.plots=FALSE)
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
load("~/R/dev/onsoilsurvey/data/profiles.rda")
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::check()
devtools::run_examples()
devtools::run_examples(oss.goof)
devtools::run_examples()
data(profiles)
data(profiles)
devtools::document()
devtools::run_examples()
devtools::document()
devtools::run_examples()
oss.optspline<- function(obj, var.name, lam= 0.1, d= t(c(0,5,15,30,60,100,200)), vlow= 0, vhigh= 1000, dir=getwd(), save.plots=TRUE){
d.dat<- obj
# create a directory for output of spline plots if user wants them
if(save.plots==TRUE){
output_dir<- paste0(dir,'/Spline_plots')
dir.create(paste0(dir,'/Spline_plots'))}
#set the first column, the site identifier, to character
d.dat[,1]<- as.character(d.dat[,1])
# split the input data frame into a list by site identifier
datlist<- split.data.frame(d.dat,d.dat[,1])
# create an empty list to hold the outputs of the function
splinelist<- list()
#create a vector of the site identifiers for naming plots
names<- unique(d.dat[,1])
#start of the outer loop that will run ea_spline for each site
for (i in 1:length(datlist)){
#split the plotting window based on number of lambda values
dims=c(round(length(lam)+1)/2,2)
par(mfrow=c(round(length(lam)+1)/2,2))
# start the inner loop which evaluates the various lambda for each profile in your list
for(j in 1:length(lam)){
#select the lambda
y<- lam[j]
# Run the spline and compile the RMSE values in a table
DataSpline <- suppressMessages(ithir::ea_spline(datlist[[i]], var.name= var.name ,d = d,lam = y, vlow=vlow, vhigh = vhigh, show.progress=FALSE))
l_eval<- cbind(y, sum(DataSpline$splineFitError$rmse, na.rm=TRUE))
ifelse(exists('lam_sum'), lam_sum<- rbind(lam_sum,l_eval), lam_sum<- l_eval)
# we need to correct the issue with numbers coming out as characters, a workaround
DataSpline$obs.preds[,2]<- as.numeric(DataSpline$obs.preds[,2])
DataSpline$obs.preds[,3]<- as.numeric(DataSpline$obs.preds[,3])
DataSpline$obs.preds[,4]<- as.numeric(DataSpline$obs.preds[,4])
# we add the plotting function here
oss.plot_spl(splineOuts= DataSpline, d = d, maxd = 100, type = 3)
mtext(paste0("lambda = ",y),cex = 0.8)
#end of inner loop for testing the specified lambda values
}
# once the loop is done we can save the plot to png if the user wants them
if(save.plots==TRUE){
dev.copy(png, paste0(output_dir,"/Spline_", var.name, "_", names[i], ".png"), width=600, height=800)
if(!is.null(dev.list())) dev.off()}
# then you retrieve the optimal lambda for the site
if (nrow(lam_sum)==1) {z= as.numeric(lam_sum[1,1])
} else {
lam_sum<- lam_sum[order(lam_sum[,2]),]
z=as.numeric(lam_sum[1,1])}
# and now run the spline one last time with optimal lambda and commit it to a new list
splinelist[[i]] <- suppressMessages(ithir::ea_spline(datlist[[i]], var.name= var.name ,d = d,lam = z, vlow=vlow, vhigh = vhigh, show.progress=FALSE))
rm(lam_sum)
#store the lambda values being used for future reference
ifelse(exists('lamdf'), lamdf<- rbind(lamdf,z), lamdf<- z)
# end of the outer loop which is based on the number of unique sites
}
# convert the table of optimal lambda values to dataframe and name the column
rownames(lamdf)<- c(seq(1,nrow(lamdf),by=1))
lamdf<- as.data.frame(lamdf)
colnames(lamdf)<- "lambda"
#small loop to unlist the data into dataframes
for (i in 1:length(splinelist)){
temp1 <- splinelist[[i]][[1]]
temp2 <- splinelist[[i]][[2]]
temp3 <- splinelist[[i]][[3]]
temp4 <- splinelist[[i]][[4]]
ifelse(exists('harmonized'), harmonized<- rbind(harmonized,temp1), harmonized<- temp1)
ifelse(exists('obs'), obs<- rbind(obs,temp2), obs<- temp2)
ifelse(exists('rmse'), rmse<- rbind(rmse,temp3), rmse<- temp3)
ifelse(exists('var.1cm'), var.1cm<- cbind(var.1cm,temp4), var.1cm<- temp4)
colnames(harmonized)<- colnames(splinelist[[i]][[1]])
colnames(obs)<- colnames(splinelist[[i]][[2]])
}
# combine the dataframes into an output identical to ea_spline list object, except we add the lambda table
datlist<- list(harmonized, obs, rmse, var.1cm,lamdf)
names(datlist)<- c('harmonised', 'obs.preds', 'splineFitError','var.1cm','lambda')
datlist$obs.preds$SiteID<- as.factor(datlist$obs.preds$SiteID)
#clean up the working environment
rm(harmonized,obs,var.1cm,rmse,temp1,temp2,temp3,temp4,DataSpline,l_eval,splinelist,d.dat)
return(datlist)}
oss.optspline(obj=profiles,var.name="CEC",lam=lam,d=d,save.plots=FALSE)
data(profiles)
d=t(c(0,5,15,30,60,100))
lam=c(1, 0.1, 0.01)
oss.optspline(obj=profiles,var.name="CEC",lam=lam,d=d,save.plots=FALSE)
lam=c(10, 1, 0.1, 0.01, 0.001)
oss.optspline(obj=profiles,var.name="CEC",lam=lam,d=d,save.plots=FALSE)
oss.optspline(obj=profiles,var.name="CEC",lam=lam,d=d,save.plots=TRUE)
is(obj,"SoilProfileCollection") == TRUE
obj=profiles
is(obj,"SoilProfileCollection") == TRUE
is(obj,"data.frame") == TRUE
devtools::document()
devtools::document()
devtools::run_examples()
devtools::load_all()
devtools::check()
devtools::document()
devtools::check()
